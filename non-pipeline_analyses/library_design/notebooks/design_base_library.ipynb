{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7b80bf-2b3b-42e9-9650-44171039d20c",
   "metadata": {},
   "source": [
    "# Design base library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9778e5-5b7a-4fd9-8ae7-b8152fd26af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5fff3e-a6f2-4310-a685-8ce188e26e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarydir = '../results/summary_tables'\n",
    "os.makedirs(summarydir, exist_ok=True)\n",
    "\n",
    "os.makedirs(\"../results/selected_library_strains\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da74e4-b019-49a4-8b60-7790ae43ba21",
   "metadata": {},
   "source": [
    "## Load summary tables of strains from the GISAID download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b272673-ce2b-4b99-84f6-651b015a1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h3 = pd.read_csv('../data/accessions_to_download/H3_candidate_summary.csv')\n",
    "all_h1 = pd.read_csv('../data/accessions_to_download/H1_candidate_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2e6ab-cb4d-4704-aa93-2d1a5a8332d6",
   "metadata": {},
   "source": [
    "## Process aligned sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8e574c-8155-4f53-8a68-acae764709c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = [\n",
    "    'h1n1pdm', 'h3n2', \n",
    "]\n",
    "\n",
    "# First 6 bp of HA coding sequence\n",
    "motif = \"atgaag\"\n",
    "\n",
    "# Iterate through each sequence\n",
    "for alignment in alignments:\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    for record in SeqIO.parse(f'../results/alignments/2025-04-07_{alignment}_ha_aligned.fasta', \"fasta\"):\n",
    "        sequence = str(record.seq).lower()\n",
    "        positions = []\n",
    "    \n",
    "        # Search for all occurrences of the motif\n",
    "        # Use to find the start and end index\n",
    "        start_index = sequence.find(motif)\n",
    "        end_index = start_index+1500\n",
    "\n",
    "        trimmed_ha_nuc = (sequence[start_index:end_index])\n",
    "        trimmed_ha_ectodomain_prot = (Seq(trimmed_ha_nuc, 'unambiguous_dna').translate())\n",
    "\n",
    "        if alignment == 'h1n1pdm':\n",
    "            trimmed_ha_ha1_prot = trimmed_ha_ectodomain_prot[17:345] # Trim for H1\n",
    "        elif alignment == 'h3n2':\n",
    "            trimmed_ha_ha1_prot = trimmed_ha_ectodomain_prot[20:348] # Trim for H3\n",
    "\n",
    "        strain, gisaid_id = (record.id.split('|'))\n",
    "\n",
    "        sequences.append([strain, gisaid_id,trimmed_ha_ha1_prot])\n",
    "\n",
    "    outfile = os.path.join(summarydir, f'{alignment}_aligned_ha1.csv')\n",
    "    pd.DataFrame(sequences, columns = ['name', 'accession_ha', 'ha1_sequence']).to_csv(outfile, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b00d19-8725-47e2-b057-ae5e21b9945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID map\n",
    "# Need to know the isolate ID to HA ID\n",
    "h1_map = pd.read_csv('../data/sequences/h1_id_map.csv')\n",
    "h3_map = pd.read_csv('../data/sequences/h3_id_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16936b-fca0-45ef-8962-db6f75c27bb3",
   "metadata": {},
   "source": [
    "For each unique HA1 sequence, see which methods picked each sequence. We want some convergence but not total convergence, since each of the methods are looking at slightly different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f95862-111d-4e44-9004-0257662ffd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in alignment\n",
    "h3 = pd.read_csv(os.path.join(summarydir,'h3n2_aligned_ha1.csv'))\n",
    "\n",
    "all_h3 = (all_h3\n",
    "          .merge(h3_map.rename(columns = {'ha_id': 'accession_ha'})) # get HA ID to isolate ID\n",
    "          .merge(h3.rename(columns = {'accession_ha': 'isolate_id'})) \n",
    "          # .rename(columns = {'isolate_id': 'gisaid_id'})\n",
    "         )\n",
    "\n",
    "# Initialize empty list for matching HA1 sequences to methods \n",
    "ha1_to_method = []\n",
    "\n",
    "for ha1_sequence in (all_h3.ha1_sequence.unique()):\n",
    "    \n",
    "\n",
    "    temp_df = (all_h3.query(f'ha1_sequence == \"{ha1_sequence}\"'))\n",
    "    method = (temp_df.method.to_list())\n",
    "\n",
    "    ha1_to_method.append([ha1_sequence,method])\n",
    "\n",
    "    \n",
    "all_h3_tidy = (all_h3\n",
    "               .merge(h3.rename(columns = {'accession_ha': 'isolate_id'}))\n",
    "               .merge(pd.DataFrame(ha1_to_method, columns = ['ha1_sequence', 'methods']))\n",
    "               .drop(columns=['method', 'mutations', 'haplotype'])\n",
    "               .drop_duplicates(subset=['ha1_sequence'], keep='first')\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "\n",
    "# pd.DataFrame(ha1_to_method, columns = ['ha1_sequence', 'methods'])\n",
    "\n",
    "all_h3_tidy.to_csv(os.path.join(summarydir, '2025-04-07_h3_sequences.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c628a7e-6114-4da9-b821-4193856918a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = pd.read_csv(os.path.join(summarydir, 'h1n1pdm_aligned_ha1.csv'))\n",
    "\n",
    "all_h1 = (all_h1\n",
    "          .merge(h1_map.rename(columns = {'ha_id': 'accession_ha'})) # get HA ID to isolate ID\n",
    "          .merge(h1.rename(columns = {'accession_ha': 'isolate_id'})) \n",
    "          # .rename(columns = {'isolate_id': 'gisaid_id'})\n",
    "         )\n",
    "\n",
    "ha1_to_method = []\n",
    "\n",
    "for ha1_sequence in (all_h1.ha1_sequence.unique()):\n",
    "\n",
    "    temp_df = (all_h1.query(f'ha1_sequence == \"{ha1_sequence}\"'))\n",
    "    method = (temp_df.method.to_list())\n",
    "\n",
    "    ha1_to_method.append([ha1_sequence,method])\n",
    "\n",
    "    \n",
    "all_h1.merge(h1.rename(columns = {'accession_ha': 'isolate_id'}))\n",
    "\n",
    "all_h1_tidy = (all_h1\n",
    "               .merge(h1.rename(columns = {'accession_ha': 'isolate_id'}))\n",
    "               .merge(pd.DataFrame(ha1_to_method, columns = ['ha1_sequence', 'methods']))\n",
    "               .drop(columns=['method', 'mutations', 'haplotype'])\n",
    "               .drop_duplicates(subset=['ha1_sequence'], keep='first')\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "\n",
    "# pd.DataFrame(ha1_to_method, columns = ['ha1_sequence', 'methods'])\n",
    "\n",
    "all_h1_tidy.to_csv(os.path.join(summarydir, '2025-04-07_h1_sequences.csv'), index=False)\n",
    "# all_h1_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac4ae1-0447-4393-9aa2-a7a3c0f7663e",
   "metadata": {},
   "source": [
    "## Compare strains in H3 and H1 libraries to list of strains used to vaccinate ferrets\n",
    "We want HA1 exact matches to these strains. If they don't exist in the library we should add them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d19d11-5893-4ac1-a1c5-ab93ebb1f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = [\n",
    "    'ferret_strains_h3',\n",
    "    'ferret_strains_h1'\n",
    "\n",
    "]\n",
    "\n",
    "# First 6 bp of HA coding sequence\n",
    "motif = \"atgaag\"\n",
    "\n",
    "# Iterate through each sequence\n",
    "for alignment in alignments:\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    for record in SeqIO.parse(f'../results/alignments/2025-04-08_{alignment}.fasta', \"fasta\"):\n",
    "        sequence = str(record.seq).lower()\n",
    "        positions = []\n",
    "    \n",
    "        # Search for all occurrences of the motif\n",
    "        # Use to find the start and end index\n",
    "        start_index = sequence.find(motif)\n",
    "        end_index = start_index+1500\n",
    "\n",
    "        trimmed_ha_nuc = (sequence[start_index:end_index])\n",
    "        trimmed_ha_ectodomain_prot = (Seq(trimmed_ha_nuc, 'unambiguous_dna').translate())\n",
    "\n",
    "        if alignment == 'ferret_strains_h1':\n",
    "            trimmed_ha_ha1_prot = trimmed_ha_ectodomain_prot[17:345] # Trim for H1\n",
    "        elif alignment == 'ferret_strains_h3':\n",
    "            trimmed_ha_ha1_prot = trimmed_ha_ectodomain_prot[20:348] # Trim for H3\n",
    "\n",
    "        strain, gisaid_id = (record.id.split('|'))\n",
    "\n",
    "        sequences.append([strain, gisaid_id,trimmed_ha_ha1_prot])\n",
    "\n",
    "    outfile = os.path.join(summarydir, f'{alignment}_aligned_ha1.csv')\n",
    "    pd.DataFrame(sequences, columns = ['name', 'accession_ha', 'ha1_sequence']).to_csv(outfile, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336ee5d2-fef5-4389-a26c-bd7a907b2cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accession_ha</th>\n",
       "      <th>ha1_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/Norway/07606/2024</td>\n",
       "      <td>EPI_ISL_19723534</td>\n",
       "      <td>DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/Lisboa/188/2023</td>\n",
       "      <td>EPI_ISL_18950107</td>\n",
       "      <td>DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/Tajikistan/02-1057/2024</td>\n",
       "      <td>EPI_ISL_19440507</td>\n",
       "      <td>DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLKG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name      accession_ha  \\\n",
       "0        A/Norway/07606/2024  EPI_ISL_19723534   \n",
       "1          A/Lisboa/188/2023  EPI_ISL_18950107   \n",
       "2  A/Tajikistan/02-1057/2024  EPI_ISL_19440507   \n",
       "\n",
       "                                        ha1_sequence  \n",
       "0  DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...  \n",
       "1  DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...  \n",
       "2  DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLKG...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ferret_h3 = pd.read_csv(os.path.join(summarydir, 'ferret_strains_h3_aligned_ha1.csv'))\n",
    "ferret_h1 = pd.read_csv(os.path.join(summarydir, 'ferret_strains_h1_aligned_ha1.csv'))\n",
    "\n",
    "\n",
    "library_ha1s = pd.concat([h3, h1]).ha1_sequence.to_list()\n",
    "\n",
    "\n",
    "\n",
    "ferret_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa83183-0199-489d-8921-3f175ec4752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ha1s = []\n",
    "\n",
    "for ha1 in pd.concat([ferret_h3,ferret_h1]).ha1_sequence.unique():\n",
    "    if ha1 in library_ha1s:\n",
    "        pass\n",
    "    else:\n",
    "        missing_ha1s.append(ha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd6e3a2-1f12-4bac-939b-baa2889d5686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accession_ha</th>\n",
       "      <th>ha1_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A/Netherlands/10685/2024</td>\n",
       "      <td>EPI_ISL_19654875</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/Norway/07606/2024</td>\n",
       "      <td>EPI_ISL_19723534</td>\n",
       "      <td>DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/Tajikistan/02-1057/2024</td>\n",
       "      <td>EPI_ISL_19440507</td>\n",
       "      <td>DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLKG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name      accession_ha  \\\n",
       "3   A/Netherlands/10685/2024  EPI_ISL_19654875   \n",
       "0        A/Norway/07606/2024  EPI_ISL_19723534   \n",
       "2  A/Tajikistan/02-1057/2024  EPI_ISL_19440507   \n",
       "\n",
       "                                        ha1_sequence  \n",
       "3  GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...  \n",
       "0  DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRG...  \n",
       "2  DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLKG...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library_df = pd.concat([h3, h1])\n",
    "ferret_df = pd.concat([ferret_h3, ferret_h1])\n",
    "\n",
    "ferret_df[~ferret_df['ha1_sequence'].isin(missing_ha1s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0102668c-c283-4b3b-b1eb-17eb9fa7dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKNESFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLNNIYPAQNVTMPNKERFDKLYIWGVHHPDTDRNQFSLFAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKDENFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLNNIYPAQNVTMPNKEQFDKLYIWGVHHPDTDKNQFSLFAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKNESFNWTGVKQNGTSSACKRRSSSSFFSRLNWLTSLNNIYPAQNVTMPNKERFDKLYIWGVHHPDTDKNQFSLFAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKNESFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLNNIYPAQNVTMPNKERFDKLYIWGVHHPDTDRNQFSLYAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKVCNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKDENFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLNNIYPAQNVTMPNKEQFDKLYIWGVHHPDTDKNQFSLFAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGNCTLIDALLGDPQCDGFQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKDESFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLKNIYPAQNVTMPNKEQFDKLYIWGVHHPDTDKNQFSLFAQSSGRITVSTKRSQQAVIPNIGSRPRVRDIPSRISIYWTIVKPGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYVKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGKICNSPHQILDGGKCTLIDALLGDPQCDGLQNKEWDLFVERSRANSSCYPYDVPDYASLRSLVASSGTLEFKDESFNWTGVKQNGTSSACKRGSSSSFFSRLNWLTSLNNIYPAQNVTMPNKEQFDKLYIWGVHHPDTDRNQFSLFAQSSGRITVSTKISQQAVIPNIGSRPRVRDIPSRISIYWTIVKSGDILLINSTGNLIAPRGYFKIRSGKSSIMRSDAPIGECKSECITPNGSIPNDKPFQNVNRITYGACPRYIKQSTLKLATGMRNVPEKQTRGIF',\n",
       " 'DTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRGVAPLHLGQCNIAGWILGNPECESLSTARSWSYIVETSNSDNGTCYPGDFINYEELREQLSSVSSFERFEIFPKASSWPNHDSDNGVTAACPHAGAKSFYKNLIWLVKKGKSYPKINQTYINDQGKEVLVLWGIHHPPTITDQESLYQNADAYVFVGTSRYSKKFKPEIAARPKVRDQAGRMNYYWTLVEPGDKITFEATGNLVAPRYAFTMEKDAGSGIIISDTPVHDCNTTCQTPEGAINTSLPFQNVHPITIGKCPKYVRSTKLRLATGLRNVPSIQSRG']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_ha1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b43989-5747-4e3e-a900-de7bd83f993e",
   "metadata": {},
   "source": [
    "## Haplotype caller\n",
    "I want to call mutations from consensus so I have an easier way to look at all my strains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff91c6fc-bc6e-4054-8cb5-61575c8d6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coding_window(seq, start_pattern=\"ATGAAG\", window_size=1500):\n",
    "    \"\"\"Extract window starting from first instance of start_pattern.\"\"\"\n",
    "    seq = seq.upper()\n",
    "    start_index = seq.find(start_pattern.upper())\n",
    "    if start_index == -1:\n",
    "        return None  # Could not find pattern\n",
    "    return seq[start_index:start_index + window_size]\n",
    "\n",
    "def translate_sequences(dna_seqs):\n",
    "    \"\"\"Translate DNA sequences into protein sequences (1st frame).\"\"\"\n",
    "    return [str(Seq(dna).translate(to_stop=False)) for dna in dna_seqs]\n",
    "\n",
    "def get_consensus_sequence(protein_seqs):\n",
    "    \"\"\"Compute consensus sequence from a list of aligned protein sequences.\"\"\"\n",
    "    min_len = min(len(seq) for seq in protein_seqs)\n",
    "    consensus = \"\"\n",
    "    for i in range(min_len):\n",
    "        column = [seq[i] for seq in protein_seqs if len(seq) > i]\n",
    "        most_common = Counter(column).most_common(1)[0][0]\n",
    "        consensus += most_common\n",
    "    return consensus\n",
    "\n",
    "def get_mutations(seq, consensus, offset=16):\n",
    "    \"\"\"Return amino acid mutations relative to consensus, starting at position `offset + 1` as position 1.\"\"\"\n",
    "    mutations = []\n",
    "    for i in range(offset, min(len(seq), len(consensus))):\n",
    "        cons_aa = consensus[i]\n",
    "        seq_aa = seq[i]\n",
    "        if cons_aa != seq_aa and seq_aa != '*':\n",
    "            mutations.append(f\"{cons_aa}{i - offset + 1}{seq_aa}\")\n",
    "    return mutations\n",
    "\n",
    "\n",
    "def annotate_aa_mutations_from_dna_fastas(fasta_paths):\n",
    "    if isinstance(fasta_paths, str):\n",
    "        fasta_paths = [fasta_paths]\n",
    "\n",
    "    records = []\n",
    "    for path in fasta_paths:\n",
    "        for rec in SeqIO.parse(path, \"fasta\"):\n",
    "            trimmed = extract_coding_window(str(rec.seq))\n",
    "            if trimmed and len(trimmed) >= 3:\n",
    "                rec.seq = Seq(trimmed)\n",
    "                rec.id = f\"{os.path.basename(path)}|{rec.id}\"\n",
    "                records.append(rec)\n",
    "            else:\n",
    "                print(f\"Warning: {rec.id} in {path} was skipped (no valid window)\")\n",
    "\n",
    "    names = [rec.id for rec in records]\n",
    "    dna_seqs = [str(rec.seq) for rec in records]\n",
    "\n",
    "    if not dna_seqs:\n",
    "        print(\"No valid sequences found.\")\n",
    "        return []\n",
    "\n",
    "    protein_seqs = translate_sequences(dna_seqs)\n",
    "    consensus = get_consensus_sequence(protein_seqs)\n",
    "\n",
    "    annotated = []\n",
    "    for name, prot_seq in zip(names, protein_seqs):\n",
    "        muts = get_mutations(prot_seq, consensus)\n",
    "        strain_name = name.split('|')[1]\n",
    "        source = name.split('|')[0]\n",
    "        accession = name.split('|')[2]\n",
    "        annotated.append([strain_name, source, accession, muts])\n",
    "\n",
    "    return annotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94c6b2-782f-4e30-ac3b-63b8fbf743af",
   "metadata": {},
   "source": [
    "## Remove some H3 sequences on recommendation by ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65dbf02e-dc26-4e3d-9b76-31de1b89b589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>div</th>\n",
       "      <th>num_date</th>\n",
       "      <th>clade_membership</th>\n",
       "      <th>subclade</th>\n",
       "      <th>accession_ha</th>\n",
       "      <th>country</th>\n",
       "      <th>lbi</th>\n",
       "      <th>date</th>\n",
       "      <th>isolate_id</th>\n",
       "      <th>ha1_sequence</th>\n",
       "      <th>methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A/Indonesia/BIOKES-IMDN985/2024</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>2024.780</td>\n",
       "      <td>2a.3a.1</td>\n",
       "      <td>J.2.2</td>\n",
       "      <td>EPI4071556</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>0.464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPI_ISL_19769191</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[CK, JH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A/Texas/ISC-1316/2025</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>2025.084</td>\n",
       "      <td>2a.3a.1</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI4049461</td>\n",
       "      <td>Usa</td>\n",
       "      <td>0.442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPI_ISL_19759737</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[CK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A/Minnesota/141/2024</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>2024.862</td>\n",
       "      <td>2a.3a.1</td>\n",
       "      <td>J.2.2</td>\n",
       "      <td>EPI3758592</td>\n",
       "      <td>Usa</td>\n",
       "      <td>0.437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPI_ISL_19646927</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSISK...</td>\n",
       "      <td>[CK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A/Texas/15527/2024</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>2024.933</td>\n",
       "      <td>2a.3a.1</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI4096405</td>\n",
       "      <td>Usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPI_ISL_19776814</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTVVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[AL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A/Hungary/335/2024</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>2024.960</td>\n",
       "      <td>2a.3a.1</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI3861657</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPI_ISL_19695255</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[AL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>A/Tennessee/95/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI3731297</td>\n",
       "      <td>Usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>EPI_ISL_19628046</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[JH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>A/Wisconsin/NIRC-IS-1125/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI4150712</td>\n",
       "      <td>Usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>EPI_ISL_19792606</td>\n",
       "      <td>GNDNSTATLCLGHHAVPNGTIVKTITNARIEVTNATELVQNSSMGK...</td>\n",
       "      <td>[JH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>A/Netherlands/01500/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J.2</td>\n",
       "      <td>EPI4120795</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>EPI_ISL_19786780</td>\n",
       "      <td>GNGNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...</td>\n",
       "      <td>[JH]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name     div  num_date clade_membership  \\\n",
       "7   A/Indonesia/BIOKES-IMDN985/2024  0.0307  2024.780          2a.3a.1   \n",
       "12            A/Texas/ISC-1316/2025  0.0347  2025.084          2a.3a.1   \n",
       "14             A/Minnesota/141/2024  0.0352  2024.862          2a.3a.1   \n",
       "25               A/Texas/15527/2024  0.0330  2024.933          2a.3a.1   \n",
       "33               A/Hungary/335/2024  0.0296  2024.960          2a.3a.1   \n",
       "48              A/Tennessee/95/2024     NaN       NaN              NaN   \n",
       "50    A/Wisconsin/NIRC-IS-1125/2025     NaN       NaN              NaN   \n",
       "52         A/Netherlands/01500/2025     NaN       NaN              NaN   \n",
       "\n",
       "   subclade accession_ha      country    lbi        date        isolate_id  \\\n",
       "7     J.2.2   EPI4071556    Indonesia  0.464         NaN  EPI_ISL_19769191   \n",
       "12      J.2   EPI4049461          Usa  0.442         NaN  EPI_ISL_19759737   \n",
       "14    J.2.2   EPI3758592          Usa  0.437         NaN  EPI_ISL_19646927   \n",
       "25      J.2   EPI4096405          Usa    NaN         NaN  EPI_ISL_19776814   \n",
       "33      J.2   EPI3861657      Hungary    NaN         NaN  EPI_ISL_19695255   \n",
       "48      J.2   EPI3731297          Usa    NaN  2024-11-02  EPI_ISL_19628046   \n",
       "50      J.2   EPI4150712          Usa    NaN  2025-03-01  EPI_ISL_19792606   \n",
       "52      J.2   EPI4120795  Netherlands    NaN  2025-03-10  EPI_ISL_19786780   \n",
       "\n",
       "                                         ha1_sequence   methods  \n",
       "7   GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...  [CK, JH]  \n",
       "12  GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...      [CK]  \n",
       "14  GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSISK...      [CK]  \n",
       "25  GNDNSTATLCLGHHAVPNGTVVKTITNDRIEVTNATELVQNSSIGK...      [AL]  \n",
       "33  GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...      [AL]  \n",
       "48  GNDNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...      [JH]  \n",
       "50  GNDNSTATLCLGHHAVPNGTIVKTITNARIEVTNATELVQNSSMGK...      [JH]  \n",
       "52  GNGNSTATLCLGHHAVPNGTIVKTITNDRIEVTNATELVQNSSIGK...      [JH]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare fasta file of sequences to a consensus\n",
    "# Name haplotypes based on mutations from consensus\n",
    "# Eg HA1 S145N_X276E\n",
    "\n",
    "ST_sequences_to_remove = [\n",
    "    'A/Netherlands/01500/2025',\n",
    "    'A/Wisconsin/NIRC-IS-1125/2025',\n",
    "    'A/Minnesota/141/2024',\n",
    "    'A/Indonesia/BIOKES-IMDN985/2024',\n",
    "    # Personal removes\n",
    "    'A/Texas/15527/2024',\n",
    "    'A/Tennessee/95/2024',\n",
    "    'A/Hungary/335/2024',\n",
    "    'A/Texas/ISC-1316/2025'\n",
    "]\n",
    "\n",
    "\n",
    "ST_sequences_to_add = [\n",
    "    'EPI_ISL_19754645',\n",
    "    'EPI_ISL_19755351',\n",
    "    'EPI_ISL_19789808',\n",
    "    'EPI_ISL_19266713',\n",
    "    'EPI_ISL_19769038',\n",
    "    'EPI_ISL_19720065',\n",
    "    'EPI_ISL_19708803',\n",
    "    'EPI_ISL_19754053',\n",
    "    'EPI_ISL_19775350',\n",
    "    'EPI_ISL_19790412',\n",
    "    'EPI_ISL_19777744',\n",
    "    'EPI_ISL_19731877',\n",
    "]\n",
    "\n",
    "all_h3_tidy[all_h3_tidy['name'].isin(ST_sequences_to_remove)]\n",
    "# all_h3_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a29bb3-d02f-4f82-832e-12b2ed2b89ce",
   "metadata": {},
   "source": [
    "Make trimmed alignments of H3 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7ded4e-3a98-4b38-a59f-8b6f0a7b565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched sequences saved to ../results/selected_library_strains/h3_strains_prot.fasta\n"
     ]
    }
   ],
   "source": [
    "# Get ferret haplotypes\n",
    "# Compare to library and Sam's recommended strains\n",
    "# Prioritize those strains with overlap\n",
    "\n",
    "output = annotate_aa_mutations_from_dna_fastas([\n",
    "    '../results/alignments/2025-04-07_h3n2_ha_aligned.fasta', \n",
    "    '../results/alignments/2025-04-08_ferret_strains_h3.fasta', \n",
    "    '../results/alignments/ST_h3n2_metadata.fasta'\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "all_h3_tidy_mutations = pd.DataFrame(output, columns = ['name','fasta','accession','mutations']).merge(\n",
    "    all_h3_tidy, \n",
    "    on='name', \n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# Remove Sam's recommended eliminations\n",
    "all_h3_tidy_mutations = all_h3_tidy_mutations[~all_h3_tidy_mutations['name'].isin(ST_sequences_to_remove)]\n",
    "\n",
    "# Get list of unique mutation sets\n",
    "mut_list = []\n",
    "for mut in all_h3_tidy_mutations.mutations:\n",
    "    if mut not in mut_list:\n",
    "        mut_list.append(mut)\n",
    "\n",
    "# Choose single strain for each mutation set with multiple strains\n",
    "strains_to_keep = ['A/Washington/284/2024', 'A/Washington/15245/2025', 'A/Canberra/613/2024', 'A/Texas/15550/2024',\n",
    "                   'A/Texas/ISC-1148/2025', 'A/Colombia/1851/2024', 'A/Netherlands/01502/2025', 'A/Tasmania/836/2024',\n",
    "                   'A/France/PAC-RELAB-HCL024172122101/2024',\n",
    "                   'A/Switzerland/860423897313/2023', 'A/Queensland/IN000692/2024', 'A/Massachusetts/93/2024', \n",
    "                   'A/Texas/ISC-1274/2025',\n",
    "                   'A/Indiana/46/2024'\n",
    "                  ]\n",
    "selected_strains = []\n",
    "for mut in mut_list:\n",
    "    temp_df = all_h3_tidy_mutations[all_h3_tidy_mutations['mutations'].apply(lambda x: x == mut)]\n",
    "\n",
    "    if len(temp_df) >= 2: # For mutation sets with more than one strain\n",
    "        # print('\\nMatching mutation sets idenfied for these strains...')\n",
    "        # print(temp_df)\n",
    "\n",
    "        names = temp_df.name.to_list()\n",
    "\n",
    "        for name in names:\n",
    "            if name in strains_to_keep:\n",
    "                selected_strains.append(name)\n",
    "\n",
    "    elif len(temp_df) == 1: # For mutation sets with one strain\n",
    "        selected_strains.append(temp_df.name.values[0])\n",
    "\n",
    "\n",
    "# Save dataframes of the strain names, accessions, and haplotypes\n",
    "all_h3_tidy_mutations = all_h3_tidy_mutations[all_h3_tidy_mutations['name'].isin(selected_strains)].reset_index(drop=True)\n",
    "\n",
    "all_h3_tidy_mutations[['name', 'mutations']].to_csv('../results/selected_library_strains/h3_strains.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Save the list of FASTA files for library \n",
    "input_fastas = [\n",
    "    '../results/alignments/2025-04-07_h3n2_ha_aligned.fasta', \n",
    "    '../results/alignments/2025-04-08_ferret_strains_h3.fasta', \n",
    "    '../results/alignments/ST_h3n2_metadata.fasta'\n",
    "]\n",
    "\n",
    "# Initialize list of targets\n",
    "target_accessions = all_h3_tidy_mutations.accession.to_list()  \n",
    "\n",
    "# Store all matching sequences\n",
    "matched_nucleotide_seqs = []\n",
    "matched_protein_seqs = []\n",
    "\n",
    "# Define the custom start codon to search for\n",
    "start_codon = \"atgaag\"\n",
    "\n",
    "# Loop through each specified FASTA file\n",
    "for fasta_file in input_fastas:\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            # Find the position of the first instance of 'ATGAAG'\n",
    "            start_position = record.seq.find(start_codon)\n",
    "            \n",
    "            if start_position == -1:\n",
    "                print(f\"Skipping {record.id} - no 'atgaag' found\")\n",
    "                continue  # Skip if 'ATGAAG' is not found\n",
    "            \n",
    "            # Extract the sequence starting from the found position\n",
    "            end_position = start_position + 1602\n",
    "            nucleotide_seq = record.seq[start_position:end_position]\n",
    "            translated_seq = nucleotide_seq.translate()\n",
    "\n",
    "            # Make separate SeqRecords for nucleotide and protein\n",
    "            nucleotide_record = SeqRecord(nucleotide_seq, id=record.id, description=record.description)\n",
    "            protein_record = SeqRecord(translated_seq, id=record.id, description=record.description)\n",
    "\n",
    "            # Check if the sequence ID matches any target accessions\n",
    "            if record.id.split('|')[1] in target_accessions:\n",
    "                matched_nucleotide_seqs.append(nucleotide_record)\n",
    "                matched_protein_seqs.append(protein_record)\n",
    "\n",
    "                \n",
    "# Write all matched protein sequences to a new FASTA file, remove GISAID IDs \n",
    "h3_output_file = \"../results/selected_library_strains/h3_strains_prot.fasta\"\n",
    "with open(h3_output_file, \"w\") as output_handle:\n",
    "    for seq in matched_protein_seqs:\n",
    "        output_handle.write(f\">{seq.id.split('|')[0]}_H3N2\\n{str(seq.seq)}\\n\")\n",
    "\n",
    "print(f\"Matched sequences saved to {h3_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3fb5b-9c8c-450d-a1b5-e8378235369e",
   "metadata": {},
   "source": [
    "## Repeat the above analysis for H1s\n",
    "Compare our list of H1s to the list of strains used to vaccinate ferrets. ST made no recommendations for H1s so we can skip that part. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c93be-e4be-4db6-8f67-95ad563f542f",
   "metadata": {},
   "source": [
    "### Add recent high frequency haplotype H1s from circulating analysis\n",
    "\n",
    "My analysis of circulating sequences showed that we were missing a lot of H1 HA1 haplotypes present in the past few months. To rectify this, I added 4 additional strains. Those strains were downloaded and placed in `../data/sequences/2025-04-25_additional_H1s.fasta` and aligned in `../results/alignments/2025-04-25_additional_H1s.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ab4e3c-fd7c-40a6-85ad-79933a5a7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom offset for the H1s\n",
    "def get_mutations(seq, consensus, offset=17):\n",
    "    \"\"\"Return amino acid mutations relative to consensus, starting at position `offset + 1` as position 1.\"\"\"\n",
    "    mutations = []\n",
    "    for i in range(offset, min(len(seq), len(consensus))):\n",
    "        cons_aa = consensus[i]\n",
    "        seq_aa = seq[i]\n",
    "        if cons_aa != seq_aa and seq_aa != '*':\n",
    "            mutations.append(f\"{cons_aa}{i - offset + 1}{seq_aa}\")\n",
    "    return mutations\n",
    "\n",
    "output = annotate_aa_mutations_from_dna_fastas([\n",
    "    '../results/alignments/2025-04-07_h1n1pdm_ha_aligned.fasta', \n",
    "    '../results/alignments/2025-04-08_ferret_strains_h1.fasta', \n",
    "    '../results/alignments/2025-04-25_missing_h1s_nuc.fasta'\n",
    "])\n",
    "\n",
    "\n",
    "all_h1_tidy_mutations = pd.DataFrame(output, columns = ['name','fasta','accession','mutations']).merge(\n",
    "    all_h1_tidy, \n",
    "    on='name', \n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "h1_to_keep = all_h1_tidy.name.tolist()\n",
    "h1_to_keep.extend(['A/Saint-Petersburg/RII-04/2025', 'A/Utah/39/2025', 'A/Hawaii/ISC-1140/2025', 'A/NovaScotia/ET1801CP00018S/2025'])\n",
    "\n",
    "all_h1_tidy_mutations = all_h1_tidy_mutations[all_h1_tidy_mutations['name'].isin(h1_to_keep)].reset_index(drop=True)\n",
    "\n",
    "all_h1_tidy_mutations[['name', 'mutations']].to_csv('../results/selected_library_strains/h1_strains.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76edf014-ab39-4bd1-960a-52a4f4c1eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched sequences saved to ../results/selected_library_strains/h1_strains_prot.fasta\n"
     ]
    }
   ],
   "source": [
    "# Save the list of FASTA files for library \n",
    "input_fastas = [\n",
    "    '../results/alignments/2025-04-07_h1n1pdm_ha_aligned.fasta', \n",
    "    '../results/alignments/2025-04-08_ferret_strains_h1.fasta',\n",
    "    '../results/alignments/2025-04-25_additional_H1s.fasta'\n",
    "]\n",
    "\n",
    "# Initialize list of targets\n",
    "target_accessions = all_h1_tidy_mutations.accession.to_list()  \n",
    "\n",
    "# Store all matching sequences\n",
    "matched_nucleotide_seqs = []\n",
    "matched_protein_seqs = []\n",
    "\n",
    "# Define the custom start codon to search for\n",
    "start_codon = \"atgaag\"\n",
    "\n",
    "# Loop through each specified FASTA file\n",
    "for fasta_file in input_fastas:\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            # Find the position of the first instance of 'ATGAAG'\n",
    "            start_position = record.seq.find(start_codon)\n",
    "            \n",
    "            if start_position == -1:\n",
    "                print(f\"Skipping {record.id} - no 'atgaag' found\")\n",
    "                continue  # Skip if 'ATGAAG' is not found\n",
    "            \n",
    "            # Extract the sequence starting from the found position\n",
    "            end_position = start_position + 1602\n",
    "            nucleotide_seq = record.seq[start_position:end_position]\n",
    "            translated_seq = nucleotide_seq.translate()\n",
    "\n",
    "            # Make separate SeqRecords for nucleotide and protein\n",
    "            nucleotide_record = SeqRecord(nucleotide_seq, id=record.id, description=record.description)\n",
    "            protein_record = SeqRecord(translated_seq, id=record.id, description=record.description)\n",
    "\n",
    "            # Check if the sequence ID matches any target accessions\n",
    "            if record.id.split('|')[1] in target_accessions:\n",
    "                matched_nucleotide_seqs.append(nucleotide_record)\n",
    "                matched_protein_seqs.append(protein_record)\n",
    "\n",
    "\n",
    "\n",
    "# Write all matched protein sequences to a new FASTA file, writing only strain names\n",
    "h1_output_file = \"../results/selected_library_strains/h1_strains_prot.fasta\"\n",
    "with open(h1_output_file, \"w\") as output_handle:\n",
    "    for seq in matched_protein_seqs:\n",
    "        output_handle.write(f\">{seq.id.split('|')[0]}_H1N1\\n{str(seq.seq)}\\n\")\n",
    "\n",
    "print(f\"Matched sequences saved to {h1_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfb318-b2ef-4f27-9ef9-3d3b59045afc",
   "metadata": {},
   "source": [
    "## Concatenate the H1 and H3 protein sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b825c40-3a1e-432d-b5f4-7182a2cd42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/selected_library_strains/h3_and_h1_prots.fasta\", \"w\") as f:\n",
    "    subprocess.run(\n",
    "        [\"cat\", h3_output_file, h1_output_file],\n",
    "        check=True,\n",
    "        stdout=f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceff83d-1996-42c0-81b7-cab14867dc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
